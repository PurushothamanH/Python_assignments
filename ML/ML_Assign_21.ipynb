{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Estimated Depth of a Decision Tree Trained on a One Million Instance Training Set\n",
    "The estimated depth of a Decision Tree trained on a one million instance training set can vary widely based on several factors, including:\n",
    "\n",
    "Number of Features: More features can lead to a deeper tree as the algorithm attempts to find the best splits.\n",
    "Data Characteristics: If the data is complex and not linearly separable, the tree might be deeper to capture all interactions.\n",
    "Stopping Criteria: Without restrictions (like max depth), the tree can grow until it perfectly classifies all training instances, leading to a very deep tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f076655",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Gini Impurity of a Node Compared to its Parent\n",
    "The Gini impurity of a node is usually lower than that of its parent after a split. This is because the goal of the split is to reduce impurity, thereby creating child nodes that are more homogeneous in terms of class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd64a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Reducing Max Depth to Address Overfitting\n",
    "Yes, it is a good idea to reduce the maximum depth of a Decision Tree if it is overfitting the training set. Overfitting occurs when the model learns the noise in the training data rather than the underlying patterns. Limiting the tree depth can help generalize the model better to unseen data, thus improving performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bf3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Scaling Input Features for Underfitting Decision Trees\n",
    "Scaling input features is not necessary for Decision Trees. They are invariant to the scale of the features since they make splits based on feature values rather than distances. If a Decision Tree is underfitting, the solution may involve increasing tree complexity (like increasing the max depth, increasing min samples per leaf, etc.) rather than scaling the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2533e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Time to Train a Decision Tree on 10 Million Instances\n",
    "The training time for a Decision Tree typically does not scale linearly with the number of instances, as it also depends on factors like the number of features and the specific implementation. However, as a rough estimate:\n",
    "If it takes 1 hour to train on 1 million instances, training on 10 million instances could take approximately 10 hours if we assume linear scaling (which is often not the case). However, due to optimizations in the algorithm and potential hardware capabilities, the actual time might be less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Setting presort=True with 100,000 Instances\n",
    "Setting presort=True can speed up training for small datasets but is generally not recommended for larger datasets (like 100,000 instances) due to its inefficiency in terms of memory usage and time. Instead, the default behavior (not presorting) is usually more efficient for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d76543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# a. Build a moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# b. Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# c. Grid search for best hyperparameters\n",
    "param_grid = {'max_leaf_nodes': [10, 20, 30, 40, 50, None]}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# d. Train model with best hyperparameters\n",
    "best_tree = grid_search.best_estimator_\n",
    "best_tree.fit(X_train, y_train)\n",
    "accuracy = best_tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy.stats import mode\n",
    "\n",
    "# a. Create subsets of the training set\n",
    "n_trees = 1000\n",
    "n_instances_per_subset = 100\n",
    "shuffle_split = ShuffleSplit(n_splits=n_trees, train_size=n_instances_per_subset, random_state=42)\n",
    "\n",
    "# b. Train Decision Trees on each subset\n",
    "trees = []\n",
    "for train_index, _ in shuffle_split.split(X_train):\n",
    "    X_subset = X_train[train_index]\n",
    "    y_subset = y_train[train_index]\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=best_tree.max_leaf_nodes, random_state=42)\n",
    "    tree.fit(X_subset, y_subset)\n",
    "    trees.append(tree)\n",
    "\n",
    "# c. Make predictions for each test case\n",
    "tree_predictions = np.array([tree.predict(X_test) for tree in trees])\n",
    "majority_votes = mode(tree_predictions, axis=0)[0].flatten()\n",
    "\n",
    "# d. Evaluate predictions\n",
    "accuracy_forest = np.mean(majority_votes == y_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_forest * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b66d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6032a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
