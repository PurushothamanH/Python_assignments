{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Difference Between Supervised and Unsupervised Learning\n",
    "Supervised Learning:\n",
    "In supervised learning, the model is trained on labeled data, meaning each training example is paired with an output label. The algorithm learns the mapping from inputs to outputs, allowing it to make predictions on unseen data.\n",
    "Examples:\n",
    "Classification: Identifying if an email is spam or not based on labeled emails.\n",
    "Regression: Predicting house prices based on features like size, location, and number of rooms.\n",
    "Unsupervised Learning:\n",
    "In unsupervised learning, the model is trained on data without labeled responses. The goal is to identify patterns, structures, or groupings in the data.\n",
    "Examples:\n",
    "Clustering: Grouping customers into segments based on purchasing behavior.\n",
    "Dimensionality Reduction: Reducing the number of features in a dataset while retaining essential information (e.g., using PCA).\n",
    "\n",
    "2. Applications of Unsupervised Learning\n",
    "Customer Segmentation: Grouping customers based on behavior for targeted marketing.\n",
    "Anomaly Detection: Identifying unusual patterns that do not conform to expected behavior (e.g., fraud detection).\n",
    "Market Basket Analysis: Discovering associations between products purchased together.\n",
    "Document Clustering: Grouping similar documents or texts for information retrieval.\n",
    "Image Compression: Reducing the size of image files while retaining essential features.\n",
    "\n",
    "3. Three Main Types of Clustering Methods\n",
    "Partitioning Methods:\n",
    "Characteristics: Divide the dataset into distinct, non-overlapping subsets (clusters). Each point belongs to exactly one cluster.\n",
    "Example: K-means clustering.\n",
    "Hierarchical Methods:\n",
    "Characteristics: Build a hierarchy of clusters either by a top-down (divisive) approach or a bottom-up (agglomerative) approach. It can represent clusters at various levels of granularity.\n",
    "Example: Agglomerative clustering.\n",
    "Density-Based Methods:\n",
    "Characteristics: Identify clusters based on the density of data points in the region. It can find clusters of arbitrary shape and is resistant to noise.\n",
    "Example: DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\n",
    "\n",
    "4. Consistency of Clustering in the K-Means Algorithm\n",
    "The k-means algorithm assesses the consistency of clustering through the following:\n",
    "Centroid Calculation: After assigning points to clusters, it recalculates the centroid (mean) of each cluster. The goal is to minimize the distance between points in a cluster and the centroid.\n",
    "SSE (Sum of Squared Errors): Evaluates how well the clusters are formed. Lower SSE indicates better clustering consistency, as it shows points are closer to their respective centroids.\n",
    "\n",
    "5. Key Difference Between K-Means and K-Medoids Algorithms\n",
    "K-Means:\n",
    "Centroid-based: Uses the mean of the points in a cluster as the centroid.\n",
    "Sensitivity: Sensitive to outliers since the mean can be skewed by extreme values.\n",
    "K-Medoids:\n",
    "Medoid-based: Uses the most centrally located point (medoid) in a cluster as the center.\n",
    "Robustness: More robust to outliers, as the medoid is less influenced by extreme values.\n",
    "Illustration:\n",
    "K-Means: Points (A, B, C) cluster around a centroid calculated from the average.\n",
    "K-Medoids: The central point (e.g., point B) is selected as the medoid, and clusters are formed around it.\n",
    "\n",
    "6. Dendrogram and How It Works\n",
    "A dendrogram is a tree-like diagram used to illustrate the arrangement of the clusters formed through hierarchical clustering.\n",
    "How It Works:\n",
    "Agglomerative Clustering: Start with each data point as a cluster and iteratively merge the closest clusters based on distance.\n",
    "Create Dendrogram: As clusters are merged, they are represented as branches, with distances (or dissimilarities) between clusters shown on the vertical axis.\n",
    "Steps to Create a Dendrogram:\n",
    "Calculate pairwise distances between all data points.\n",
    "Merge clusters based on the shortest distance.\n",
    "Continue merging until all points form a single cluster.\n",
    "Plot the dendrogram with the height indicating the distance at which clusters were combined.\n",
    "\n",
    "7. Sum of Squared Errors (SSE)\n",
    "SSE measures the total squared distance between each point and its assigned cluster centroid.\n",
    "SSE=âˆ‘ğ‘–=1ğ‘˜âˆ‘ğ‘—=1ğ‘›ğ‘–(ğ‘¥ğ‘—âˆ’ğ‘ğ‘–)2\n",
    "SSE= i=1âˆ‘k  j=1âˆ‘n iâ€‹â€‹ (xâ€‹ âˆ’c iâ€‹ )2 \n",
    "where ğ‘˜ k is the number of clusters, ğ‘›ğ‘–n iâ€‹  is the number of points in cluster ğ‘–i, ğ‘¥ğ‘—x jâ€‹  is a point in cluster ğ‘–i, and ğ‘ğ‘–c iâ€‹  is the centroid of cluster ğ‘–i.\n",
    "Role in K-Means:\n",
    "SSE is used as an objective function to minimize during clustering. The algorithm iteratively adjusts the clusters to reduce SSE, aiming for compact and well-separated clusters.\n",
    "\n",
    "8. K-Means Procedure: Step-by-Step Algorithm\n",
    "Initialization: Randomly select ğ‘˜\n",
    "k initial centroids from the dataset.\n",
    "Assignment Step: Assign each data point to the nearest centroid based on Euclidean distance.\n",
    "Update Step: Recalculate centroids by taking the mean of all points assigned to each cluster.\n",
    "Convergence Check: Repeat the assignment and update steps until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "9. Single Link and Complete Link in Hierarchical Clustering\n",
    "Single Link: Measures the shortest distance between points in two clusters. It can create long, \"chain-like\" clusters since it focuses on the nearest points.\n",
    "Complete Link: Measures the maximum distance between points in two clusters. This results in more compact clusters, as it considers the furthest points in each cluster.\n",
    "\n",
    "10. Apriori Concept in Business Basket Analysis\n",
    "The Apriori algorithm is used for mining frequent itemsets and generating association rules. It reduces measurement overhead by identifying sets of items that frequently occur together in transactions.\n",
    "Example:\n",
    "In a grocery store, if the Apriori algorithm finds that \"bread\" and \"butter\" are frequently purchased together, it can help in strategic product placement or promotional bundling.\n",
    "Reduction of Measurement Overhead:\n",
    "Instead of checking every combination of items, the Apriori algorithm uses a support threshold to limit the number of itemsets considered, focusing only on those that occur above a certain frequency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
