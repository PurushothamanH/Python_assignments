{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Difference Between Dependent and Independent Variables in Linear Equations\n",
    "Dependent Variable: The outcome or response variable that the model aims to predict or explain. It depends on the independent variables.\n",
    "Independent Variable: The predictor or explanatory variable that influences or predicts the dependent variable.\n",
    "\n",
    "2. Concept of Simple Linear Regression\n",
    "Simple Linear Regression is a technique used to model the relationship between two variables by fitting a linear equation to observed data. One variable is dependent, and the other is independent.\n",
    "Example: Predicting a person’s weight (dependent variable) based on their height (independent variable).\n",
    "\n",
    "3. Slope in Linear Regression\n",
    "In linear regression, the slope represents the rate of change of the dependent variable with respect to the independent variable. Mathematically, it’s the “rise over run” and shows how much the dependent variable changes for each unit change in the independent variable.\n",
    "\n",
    "4. Slope Calculation for Points (3, 2) and (2, 2)\n",
    "Using the formula for slope (𝑚)=(𝑦2−𝑦1)/(𝑥2−𝑥1)(m)=(y 2​ −y 1​ )/(x 2​ −x 1 ):𝑚=2−22−3=0m= 2−2−2​ =0\n",
    "Answer: The slope is 0, indicating a horizontal line.\n",
    "\n",
    "5. Conditions for a Positive Slope in Linear Regression\n",
    "A positive slope occurs when the dependent variable increases as the independent variable increases. The line slopes upwards from left to right.\n",
    "\n",
    "6. Conditions for a Negative Slope in Linear Regression\n",
    "A negative slope occurs when the dependent variable decreases as the independent variable increases. The line slopes downwards from left to right.\n",
    "\n",
    "7. Multiple Linear Regression\n",
    "Multiple Linear Regression is an extension of linear regression that models the relationship between one dependent variable and two or more independent variables. It finds the best-fitting line or hyperplane in higher dimensions to predict the dependent variable.\n",
    "\n",
    "8. Sum of Squares Due to Error (SSE) in Multiple Linear Regression\n",
    "Sum of Squares due to Error (SSE) is the sum of squared differences between the observed values and the predicted values. It quantifies the unexplained variance by the model.\n",
    "\n",
    "9. Sum of Squares Due to Regression (SSR) in Multiple Linear Regression\n",
    "Sum of Squares due to Regression (SSR) represents the sum of squared differences between the predicted values and the mean of the dependent variable. It measures the variance explained by the model.\n",
    "\n",
    "10. Multicollinearity in Regression\n",
    "Multicollinearity occurs when independent variables in a regression model are highly correlated. This can make it difficult to determine the individual effect of each variable on the dependent variable and can inflate standard errors.\n",
    "\n",
    "11. Heteroskedasticity and Its Meaning\n",
    "Heteroskedasticity occurs when the variance of the errors is not constant across observations. In regression, it can cause inefficient estimates and unreliable hypothesis tests. Homoskedasticity (constant variance) is preferred for linear models.\n",
    "\n",
    "12. Ridge Regression\n",
    "Ridge Regression is a type of regularized linear regression that adds a penalty (based on the square of the magnitude of coefficients) to the loss function. This penalty term helps reduce overfitting, especially when there is multicollinearity among the predictors.\n",
    "\n",
    "13. Lasso Regression\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is another regularized regression technique that adds an L1 penalty, based on the absolute values of coefficients, to the loss function. Lasso can reduce some coefficients to zero, effectively selecting a subset of features and performing feature selection.\n",
    "\n",
    "14. Polynomial Regression\n",
    "Polynomial Regression is an extension of linear regression that models the relationship between the dependent and independent variables as an nth-degree polynomial. It captures non-linear relationships by adding polynomial terms (e.g., 𝑥2x 2 , 𝑥3x 3 ).\n",
    "\n",
    "15. Basis Function\n",
    "A Basis Function transforms input variables in a regression model to allow for more flexibility. For example, in polynomial regression, the basis functions could include terms like 𝑥x, 𝑥2x 2 , etc. It’s useful for fitting non-linear relationships in the data.\n",
    "\n",
    "16. Logistic Regression\n",
    "Logistic Regression is a classification algorithm that models the probability of a binary outcome (0 or 1) by applying a sigmoid function to a linear equation. The sigmoid function outputs probabilities between 0 and 1, which can be mapped to class labels.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
