{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Feature Engineering\n",
    "Definition: Feature engineering is the process of creating new input features from existing data to improve model performance.\n",
    "Key Aspects:\n",
    "Transformation: Scaling, normalization, and encoding categorical variables.\n",
    "Interaction: Creating new features by combining existing ones, such as multiplying or dividing features.\n",
    "Aggregation: Summarizing information, such as calculating averages, counts, or other statistics.\n",
    "Domain-Specific: Crafting features based on domain knowledge, like converting dates to â€œdays since a specific event.â€\n",
    "\n",
    "2. Feature Selection\n",
    "Definition: Feature selection is the process of identifying and selecting the most relevant features for model building.\n",
    "Aim: It aims to reduce overfitting, enhance model interpretability, and improve computational efficiency.\n",
    "Methods:\n",
    "Filter Methods: Ranks features using statistical tests, independent of the model (e.g., chi-square, mutual information).\n",
    "Wrapper Methods: Uses a model to evaluate feature subsets iteratively (e.g., forward selection, backward elimination).\n",
    "Embedded Methods: Feature selection occurs within the model training process (e.g., Lasso regularization).\n",
    "\n",
    "3. Filter and Wrapper Approaches in Feature Selection\n",
    "Filter Approach:\n",
    "How it Works: Features are selected based on statistical criteria without relying on a model.\n",
    "Pros: Computationally fast and scales well with high-dimensional data.\n",
    "Cons: Does not account for feature interactions.\n",
    "\n",
    "Wrapper Approach:\n",
    "How it Works: Evaluates subsets of features using a specific model and iteratively selects the best combination.\n",
    "Pros: Accounts for feature interactions and may yield better model performance.\n",
    "Cons: Computationally expensive, especially on large datasets.\n",
    "\n",
    "4. Feature Selection and Extraction\n",
    "i. Overall Feature Selection Process:\n",
    "Data preprocessing.\n",
    "Ranking or scoring features based on relevance.\n",
    "Choosing the top-ranked features for model building.\n",
    "ii. Principle of Feature Extraction: Reduces data dimensions by creating new features, often using techniques like PCA, which combines correlated features to form principal components that explain most variance in the data. - Example: In text data, feature extraction can convert a document into a set of topic scores. - Popular Algorithms: Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and t-SNE.\n",
    "\n",
    "5. Feature Engineering for Text Categorization\n",
    "For text categorization, feature engineering involves:\n",
    "Tokenization: Breaking text into words or phrases.\n",
    "Normalization: Converting to lowercase, removing punctuation.\n",
    "Vectorization: Representing text as vectors (e.g., using TF-IDF).\n",
    "Dimensionality Reduction: Reducing the feature space using methods like PCA or LDA.\n",
    "\n",
    "6. Cosine Similarity in Text Categorization\n",
    "Cosine similarity measures the cosine of the angle between two vectors, making it well-suited for text since it considers the direction rather than magnitude.\n",
    "Formula: \n",
    "CosineÂ Similarity =âˆ‘ğ‘ğ‘–ğ‘ğ‘–âˆ‘ğ‘ğ‘–2Ã—âˆ‘ğ‘ğ‘–2\n",
    "CosineÂ Similarity= âˆ‘a i2â€‹ â€‹ Ã— âˆ‘b i2â€‹ â€‹ âˆ‘a iâ€‹ b â€‹ â€‹ \n",
    "Calculation: For vectors (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1), \n",
    "CosineÂ Similarity=2â‹…2+3â‹…1+2â‹…0+â‹¯+1â‹…122+32+â‹¯+12Ã—22+12+â‹¯+12\n",
    "CosineÂ Similarity= 2 2 +3 2 +â‹¯+1 2â€‹ Ã— 2 2 +1 2 +â‹¯+1 2 â€‹ 2â‹…2+3â‹…1+2â‹…0+â‹¯+1â‹…1â€‹\n",
    " \n",
    "7. Hamming Distance and Similarity Measures\n",
    "i. Hamming Distance: Counts the number of positions at which corresponding bits differ.\n",
    "Formula: Hamming Distance between 10001011 and 11001111 is 2.\n",
    "ii. Jaccard Index vs. Similarity Matching Coefficient (SMC):\n",
    "Jaccard Index: Measures similarity as the intersection divided by the union of sets.\n",
    "SMC: Calculates similarity as the number of matching values divided by the total number of values.\n",
    "\n",
    "8. High-Dimensional Dataset\n",
    "A high-dimensional dataset has a large number of features relative to the number of observations, making it challenging for machine learning.\n",
    "Examples: Text data with thousands of words as features, genetic data with many genes.\n",
    "Challenges: Increased computational cost, overfitting, and interpretability.\n",
    "Solutions: Use dimensionality reduction techniques (PCA, t-SNE), feature selection, and regularization.\n",
    "\n",
    "9. Quick Notes\n",
    "PCA (Principal Component Analysis): Reduces dimensions by transforming features into a set of orthogonal components explaining maximum variance.\n",
    "Vectors: In machine learning, vectors are ordered arrays of data values, often representing features of data points in a multidimensional space.\n",
    "Embedded Technique: Integrates feature selection within the model training process, common in decision trees and regularization techniques.\n",
    "\n",
    "10. Comparisons\n",
    "Sequential Backward Exclusion vs. Sequential Forward Selection:\n",
    "Backward Exclusion: Starts with all features and iteratively removes the least relevant.\n",
    "Forward Selection: Starts with no features and adds the most relevant one by one.\n",
    "Filter vs. Wrapper Method:\n",
    "Filter: Independent of the model, faster, and used for high-dimensional data.\n",
    "Wrapper: Evaluates feature subsets with a model, slower, but may capture feature interactions.\n",
    "SMC vs. Jaccard Coefficient:\n",
    "SMC: Considers both matching presence and absence of features.\n",
    "Jaccard Coefficient: Focuses only on shared presence between features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
