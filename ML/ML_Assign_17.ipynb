{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Basic Linear Regression with Slope and Intercept\n",
    "Linear Regression is a statistical method used to model the relationship between a dependent variable 𝑌 Y and one or more independent variables 𝑋X. The basic form of a linear regression equation is𝑌=𝑎+𝑏𝑋Y=a+bX𝑎a (intercept): The point where the line crosses the Y-axis.𝑏b (slope): The rate of change in 𝑌Y for a one-unit change in 𝑋X.\n",
    "Graph Illustration:\n",
    "A straight line representing the relationship between X and 𝑌Y.The intercept 𝑎 a is where the line crosses the Y-axis, and 𝑏 b indicates the slope.\n",
    "\n",
    "2. Terms Rise, Run, and Slope in a Graph\n",
    "Rise: The change in the Y value (vertical change) as you move along the X value (horizontal change).\n",
    "Run: The change in the X value (horizontal change).\n",
    "Slope (m): The slope of the line can be defined as:\n",
    "slope (m=riserun=Δ𝑌Δslope (m)= runrise​ =ΔΔY\n",
    "Graph Explanation:\n",
    "Visualize a right triangle formed between two points on the line.\n",
    "The vertical side represents the rise, and the horizontal side represents the run.\n",
    "\n",
    "3. Slope: Linear Positive and Negative Slope Conditions\n",
    "Positive Slope: When >0b>0, the line rises from left to right.Negative Slope: When 𝑏<0<0, the line falls from left to right.\n",
    "Graph Illustration:\n",
    "Positive slope: Line moves upwards as it progresses along the X-axis.\n",
    "Negative slope: Line moves downwards as it progresses along the X-axis.\n",
    "\n",
    "4. Curvilinear Positive and Negative Slope\n",
    "Curvilinear Positive Slope: The curve slopes upward, indicating an increasing relationship between 𝑋X and 𝑌Y but at a decreasing rate.\n",
    "Curvilinear Negative Slope: The curve slopes downward, indicating a decreasing relationship between 𝑋X and 𝑌Y but at a decreasing rate.\n",
    "Graph Illustration:\n",
    "Both curves can be represented by polynomial functions, showcasing non-linear relationships.\n",
    "\n",
    "5. Maximum and Low Points of Curves\n",
    "Maximum Point: The highest point on a curve where the slope changes from positive to negative.\n",
    "Minimum Point: The lowest point on a curve where the slope changes from negative to positive.\n",
    "Graph Illustration:\n",
    "Illustrate a curve with identifiable maxima and minima points marked.\n",
    "\n",
    "6. Ordinary Least Squares (OLS) Formulas\n",
    "The OLS method estimates the parameters 𝑎a and 𝑏b by minimizing the sum of the squared residuals (differences between observed and predicted values).\n",
    "The formulas are:=𝑛(∑𝑋𝑌)−(∑𝑋)(∑𝑌)𝑛(𝑋∑𝑋)2b= n(∑X 2 )−(∑X)2 n(∑XY)−(∑X)(∑Y)​ 𝑎=∑𝑌−𝑏(𝑋𝑛a=n∑Y−b(∑X)\n",
    "\n",
    "7. Step-by-Step Explanation of the OLS Algorithm\n",
    "Collect Data: Gather data for independent variable 𝑋X and dependent variable 𝑌Y.\n",
    "Calculate Slope (b): Use the formula to calculate the slope.\n",
    "Calculate Intercept (a): Use the formula to calculate the intercept.\n",
    "Predict Values: Use the regression equation 𝑌=𝑎+𝑏𝑋Y=a+bX to make predictions.\n",
    "Evaluate Model: Assess the model fit using metrics like R-squared and residual plots.\n",
    "\n",
    "8. Regression's Standard Error with Graph\n",
    "Standard Error (SE) quantifies the accuracy of predictions made with a regression line. It is the standard deviation of the residuals (differences between observed and predicted values).𝑆𝐸∑(𝑌𝑖𝑌^𝑖2𝑛−2SE= n−2∑(Y i​ − Y  i​ )  ​ ​ \n",
    "Graph Representation:\n",
    "A graph showing the regression line with the residuals marked between the observed values and the line.\n",
    "\n",
    "9. Example of Multiple Linear Regression\n",
    "Consider a scenario where we want to predict a house price based on several factors: size (in square feet), number of bedrooms, and age of the house.\n",
    "\n",
    "The multiple linear regression equation might look like:\n",
    "Price=𝑎+𝑏1(Size)+𝑏2(Bedrooms)+𝑏3(Age)Price=a+b 1​ (Size)+b 2 (Bedrooms)+b 3​ (Age)\n",
    "\n",
    "10. Regression Analysis Assumptions and BLUE Principle\n",
    "Assumptions of Regression Analysis:\n",
    "Linearity: The relationship between independent and dependent variables is linear.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: Constant variance of residuals.\n",
    "Normality: Residuals are normally distributed.\n",
    "BLUE Principle: The Best Linear Unbiased Estimator (BLUE) states that the OLS estimates are the best (minimum variance) among the class of linear unbiased estimators.\n",
    "\n",
    "11. Major Issues with Regression Analysis\n",
    "Multicollinearity: High correlation among independent variables can distort results.\n",
    "Heteroskedasticity: Non-constant variance of errors can affect the validity of statistical tests.\n",
    "\n",
    "12. Improving Linear Regression Model Accuracy\n",
    "Feature Engineering: Create new features that better capture relationships.\n",
    "Remove Outliers: Minimize the impact of outliers on the model.\n",
    "Regularization: Use techniques like Ridge or Lasso regression to prevent overfitting.\n",
    "\n",
    "13. Polynomial Regression Model Example\n",
    "Suppose we want to model the relationship between temperature (X) and ice cream sales (Y).\n",
    "The polynomial regression equation might be:\n",
    "𝑌=𝑎+𝑏1𝑋+𝑏2Y=a+b 1​ X+b 2​ X 2 \n",
    "This captures non-linear trends, allowing for curvature in the relationship.\n",
    "\n",
    "14. Detailed Explanation of Logistic Regression\n",
    "Logistic Regression is used for binary classification. It models the probability of the default class (e.g., success vs. failure) using the logistic function:\n",
    "𝑃(=1∣𝑋)=11+𝑒−(𝑎+𝑋)P(Y=1∣X)= 1+e −(a+bX) 1\n",
    "The output is transformed into probabilities, which can be interpreted as class labels.\n",
    "\n",
    "15. Logistic Regression Assumptions\n",
    "Binary Outcome: The dependent variable is binary.\n",
    "Independence: Observations are independent of each other.\n",
    "Linear Relationship: A linear relationship exists between the logit of the outcome and the independent variables.\n",
    "\n",
    "16. Maximum Likelihood Estimation (MLE)\n",
    "Maximum Likelihood Estimation is a method used to estimate the parameters of a statistical model. MLE chooses the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "Steps in MLE:\n",
    "Define Likelihood Function: Formulate a likelihood function based on the model and data.\n",
    "Maximize the Likelihood: Use calculus or optimization techniques to find the parameter values that maximize the likelihood function.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
