{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98105f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Examples of Prior, Posterior, and Likelihood\n",
    "Prior Probability: The probability we assign to an event before any evidence or new information.\n",
    "Example: Suppose 2% of the population is allergic to a specific antibiotic. This probability (2%) is the prior probability.\n",
    "Likelihood: The probability of observing the evidence given a specific hypothesis or condition.\n",
    "Example: If we know that someone has an allergic reaction, the likelihood could be the probability that a specific symptom, like skin rash, is present given that they are allergic.\n",
    "Posterior Probability: The probability of the hypothesis given the new evidence. Itâ€™s calculated by updating the prior with the likelihood.\n",
    "Example: If a patient shows a skin rash (evidence), the posterior probability is the updated probability that this person has the allergy given the presence of the rash.\n",
    "\n",
    "2. Bayes' Theorem in Concept Learning\n",
    "Bayesâ€™ theorem aids in concept learning by updating the probability of a hypothesis as new evidence is collected. It allows us to weigh how likely each hypothesis is given both prior knowledge and observed data, refining the accuracy of the learning process over time.\n",
    "\n",
    "3. Real-life Example of NaÃ¯ve Bayes Classifier\n",
    "In spam detection, NaÃ¯ve Bayes is used to classify emails as spam or not spam. It calculates the likelihood of certain words appearing in spam vs. non-spam emails. By treating each word as conditionally independent, NaÃ¯ve Bayes combines these likelihoods to classify new emails effectively.\n",
    "\n",
    "4. NaÃ¯ve Bayes on Continuous Numeric Data\n",
    "Yes, NaÃ¯ve Bayes can handle continuous data by assuming a Gaussian (normal) distribution for each feature. For each continuous variable, a Gaussian distribution is estimated from the training data to compute probabilities.\n",
    "\n",
    "5. Bayesian Belief Networks (BBNs)\n",
    "Bayesian Belief Networks are probabilistic graphical models that represent variables and their conditional dependencies via a directed acyclic graph (DAG). Each node represents a random variable, and edges represent conditional dependencies. BBNs are useful in medical diagnosis, troubleshooting systems, and more. They can model complex dependencies but can become computationally intensive for very large networks.\n",
    "\n",
    "6. Intruder Detection Probability (Airport Example)\n",
    "Given:ğ‘ƒ(ğ´=1âˆ£ğ¼=1)=0.98P(A=1âˆ£I=1)=0.98ğ‘ƒ(ğ´=1âˆ£ğ¼=0)=0.001P(A=1âˆ£I=0)=0.001ğ‘ƒ(ğ¼1)=0.00001 P(I=1)=0.00001\n",
    "Using Bayes' theorem to calculate ğ‘ƒ(1âˆ£ğ´=1)\n",
    "P(I=1âˆ£A=1), the probability that an alarm is triggered when an individual is actually an intruder.\n",
    "\n",
    "7. Antibiotic Resistance Test (Probability Calculation)\n",
    "Given:\n",
    "False Positive Rate: ğ‘ƒ(ğ‘‡=1âˆ£ğ·0)0.01 P(T=1âˆ£D=0)=0.01\n",
    "False Negative Rate: ğ‘ƒ(=âˆ£ğ·1)=0.05 P(T=0âˆ£D=1)=0.05 ğ‘ƒ(ğ·=1)=0.02 P(D=1)=0.02\n",
    "Calculateğ‘ƒ(ğ·=1âˆ£ğ‘‡=1)\n",
    "P(D=1âˆ£T=1), the probability that a person who tests positive is actually resistant.\n",
    "\n",
    "8. Student Exam Problem Likelihood Calculation\n",
    "Likelihood of Solving the Exam Problem: Use the probabilities of solving each problem type along with their occurrence probabilities.\n",
    "ğ‘ƒ(Solve)=(0.3Ã—0.9)+(0.2Ã—0.2)+(0.5Ã—0.6)\n",
    "P(Solve)=(0.3Ã—0.9)+(0.2Ã—0.2)+(0.5Ã—0.6)\n",
    "Likelihood Given Problem of Form A: Given the problem was solved, calculate ğ‘ƒ(âˆ£Solve)P(Aâˆ£Solve) using Bayes' theorem.\n",
    "\n",
    "9. Bank CCTV System Example\n",
    "Given:Daily bank hours: 10 hours â†’â†’ 120 intervals of 5 minutes\n",
    "Probability of customer per interval: 5%\n",
    "Detection rate for customer: 99%\n",
    "False detection rate: 10%\n",
    "Daily Customer Count: Multiply the probability of a customer by the number of intervals.\n",
    "Fake and Missed Photographs: Calculate expected counts for false positives (no customer detected as customer) and false negatives (customer not detected).\n",
    "Likelihood of Customer Given Photograph: Use Bayes' theorem to calculate the probability of a customer being present given a photo.\n",
    "\n",
    "10. Conditional Probability Table for \"Won Toss\" Node in Bayesian Belief Network\n",
    "In a Bayesian belief network, the \"Won Toss\" node would represent the probability distribution over winning the toss and subsequently affect other nodes related to winning a match. The conditional probability table (CPT) reflects the probability of winning given the toss result. Each entry in this table represents the likelihood of a match outcome given whether the toss was won or lost. This assumption simplifies the model by treating \"winning the toss\" and \"winning the match\" as conditionally independent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
