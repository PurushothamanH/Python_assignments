{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8744d4d1",
   "metadata": {},
   "source": [
    "1. Applications for Different Types of RNNs:\n",
    "\n",
    "Sequence-to-Sequence (Seq2Seq): Useful in applications where a sequence input needs to map to a sequence output. Examples include machine translation, summarization, and chatbot response generation.\n",
    "Sequence-to-Vector: Ideal for cases where a sequence maps to a fixed-size output, like sentiment analysis, where a sequence of text maps to a positive or negative sentiment.\n",
    "Vector-to-Sequence: This setup works for scenarios like image captioning, where an image (vector) needs to generate a descriptive sentence (sequence).\n",
    "\n",
    "2. Input Dimensions for an RNN Layer:\n",
    "\n",
    "Inputs typically need three dimensions: (batch size, time steps, features).\n",
    "Batch size: Number of sequences processed simultaneously.\n",
    "Time steps: Length of each sequence.\n",
    "Features: Number of features per time step (e.g., dimensions in word embeddings).\n",
    "Output Dimensions depend on the RNN configuration. If return_sequences=True, it will also return three dimensions; if False, it will return two dimensions (batch size, features).\n",
    "\n",
    "3. Return Sequences in RNN Layers:\n",
    "\n",
    "For a Seq2Seq RNN, all but the final layer should have return_sequences=True, allowing each layer to pass full sequences forward.\n",
    "For a Sequence-to-Vector RNN, only the final layer should set return_sequences=False to return a single output vector.\n",
    "\n",
    "4. RNN Architecture for Time Series Forecasting:\n",
    "\n",
    "For daily univariate time series forecasting over the next seven days, a Seq2Seq RNN with return_sequences=True and possibly an LSTM or GRU would be suitable, as it can output a seven-day sequence.\n",
    "\n",
    "5. Main Difficulties When Training RNNs and Solutions:\n",
    "\n",
    "Vanishing/Exploding Gradients: Use gated RNNs like LSTMs or GRUs to handle these issues and try gradient clipping.\n",
    "Long-Term Dependencies: LSTMs or GRUs manage long dependencies better than vanilla RNNs.\n",
    "Computational Load: Reduce complexity by using sequence truncation, batch normalization, or 1D convolutional layers for feature extraction.\n",
    "\n",
    "6. LSTM Cell Architecture:\n",
    "\n",
    "An LSTM cell consists of a forget gate, input gate, and output gate, which control what information to retain, update, and pass on to the next cell, respectively.\n",
    "\n",
    "7. Using 1D Convolutional Layers in an RNN:\n",
    "\n",
    "1D convolutions are useful for extracting local patterns in sequences (e.g., phonemes in audio processing or n-grams in text) before feeding the data into the RNN.\n",
    "\n",
    "8. Classifying Videos with Neural Networks:\n",
    "\n",
    "A 3D CNN followed by an RNN (e.g., LSTM) is common for video classification, where the CNN handles spatial features for each frame and the RNN handles temporal features across frames.\n",
    "\n",
    "9. Training a Classification Model for SketchRNN Dataset:\n",
    "\n",
    "Steps:\n",
    "Load and preprocess the SketchRNN dataset from TensorFlow Datasets.\n",
    "Use an RNN or LSTM to handle the sequence input format and train it to classify the sketches based on predefined categories.\n",
    "Experiment with data augmentation techniques (e.g., sequence cropping or rotation) to improve model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
