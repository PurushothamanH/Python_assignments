{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442f543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "# No, even if you initialize all weights randomly using He initialization, you still need to ensure they are unique for each connection in the network. Initializing all weights to the same value (even if randomly generated) would cause each neuron to compute the same output, making it impossible for the model to learn effectively. Randomness in the initialization across different weights is crucial to breaking symmetry and enabling diverse learning paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b880a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it okay to initialize the bias terms to 0?\n",
    "\n",
    "# Yes, initializing bias terms to 0 is generally okay. Unlike weights, biases don’t have symmetry-breaking properties, so setting them to zero won’t prevent the model from learning effectively. However, some prefer small random values for biases, especially in certain architectures like RNNs, to kick-start the network’s learning in cases where non-zero values may be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85d78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name three advantages of the ELU activation function over ReLU.\n",
    "\n",
    "# Smoother Negative Values: ELU smoothly extends into negative values, reducing the potential for dead neurons (as seen in ReLU).\n",
    "# Less Sensitive to Vanishing Gradients: ELU can help maintain the gradient flow for negative values, addressing the vanishing gradient problem better than ReLU.\n",
    "# Self-Normalizing Effect: ELU activation can produce activations with a mean closer to zero, which can help maintain a more normalized distribution through the network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945ce53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "# ELU: Use in deep networks where you need smoother gradients for both positive and negative values. It’s ideal for networks where the vanishing gradient is a concern.\n",
    "# Leaky ReLU (and variants like Parametric ReLU): Useful when dead neurons (neurons stuck at zero) are a problem with standard ReLU.\n",
    "# ReLU: Commonly used in hidden layers of neural networks due to its simplicity and efficiency, especially for shallow networks where vanishing gradients are less of an issue.\n",
    "# tanh: Suitable when inputs may need to be centered around zero, as it outputs values between -1 and 1.\n",
    "# Logistic (sigmoid): Often used in binary classification problems for output layers to produce probabilities between 0 and 1.\n",
    "# Softmax: Useful for multi-class classification, providing a probability distribution over classes in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e60d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "\n",
    "# Setting the momentum hyperparameter very close to 1 can cause the model to overshoot the optimal solution, as the updates will retain most of the prior momentum. This can lead to large oscillations and slower convergence as the optimizer may \"overshoot\" across the solution space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83abdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name three ways you can produce a sparse model.\n",
    "\n",
    "# Pruning: Removing weights (typically those close to zero) after or during training.\n",
    "# Regularization (L1): Using L1 regularization on weights during training, which encourages weights to become zero.\n",
    "# Distillation: Training a smaller (or sparse) model to mimic the outputs of a larger, dense model, achieving similar performance with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d42c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "\n",
    "# Training: Yes, dropout slows down training as it forces the model to update weights differently in each forward pass, effectively training different subsets of neurons and adding variability.\n",
    "# Inference: No, dropout does not slow down inference, as it’s usually disabled during inference. Instead, the weights are scaled down to compensate for the lack of dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
