{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fef0c8d1",
   "metadata": {},
   "source": [
    "1. SavedModel Contents:\n",
    "\n",
    "A SavedModel in TensorFlow is a format for saving models that includes:\n",
    "Graph Structure: The model’s computation graph.\n",
    "Weights: All model parameters.\n",
    "Signatures: Functions for model inference and other tasks.\n",
    "Inspecting: Use the saved_model_cli command-line tool or tf.saved_model.load() in Python to explore contents like the graph, inputs, outputs, and signatures.\n",
    "\n",
    "2. TF Serving:\n",
    "\n",
    "Purpose: TF Serving is a tool for deploying and serving TensorFlow models in production, handling high-performance inference requests.\n",
    "Features: It supports gRPC/REST APIs, automatic model versioning, and easy deployment updates.\n",
    "Deployment Tools: Docker, Kubernetes, or cloud providers like Google Cloud Platform’s AI Platform can be used to deploy TF Serving.\n",
    "\n",
    "3. Deploying Across Multiple TF Serving Instances:\n",
    "\n",
    "Use load balancers (like those in Kubernetes or cloud services) to distribute requests among instances. For scaling, each instance can serve the same or different model versions depending on the load requirements.\n",
    "\n",
    "4. gRPC vs. REST API in TF Serving:\n",
    "\n",
    "gRPC: Preferred for high-performance, low-latency, and binary protocol needs, often in real-time or large-batch inference.\n",
    "REST: Ideal for lighter, stateless requests and wider accessibility via HTTP protocols, which are more versatile for web-based applications.\n",
    "\n",
    "5. TFLite Model Optimization:\n",
    "\n",
    "Quantization: Reduces bit precision (e.g., float32 to int8) for smaller and faster models.\n",
    "Pruning: Removes redundant weights, effectively compressing the model.\n",
    "Weight Clustering: Groups similar weights to improve compressibility and reduce memory usage.\n",
    "\n",
    "6. Quantization-Aware Training (QAT):\n",
    "\n",
    "Purpose: QAT simulates quantized weights during training to retain model accuracy after quantization, which is critical for edge devices with reduced precision.\n",
    "\n",
    "7. Model Parallelism vs. Data Parallelism:\n",
    "\n",
    "Model Parallelism: Splits a model’s layers across devices, ideal for very large models that cannot fit on a single device but can be complex to manage.\n",
    "Data Parallelism: Replicates the full model on each device and distributes data across devices; generally recommended due to its simplicity and ease of implementation.\n",
    "\n",
    "8. Distributed Training Strategies:\n",
    "\n",
    "Data Parallel Strategies: MirroredStrategy (multi-GPU single node), MultiWorkerMirroredStrategy (multi-node).\n",
    "Parameter Server Strategy: For large, complex models where parameters are distributed across multiple servers.\n",
    "Choosing a Strategy: Use MirroredStrategy for simpler, multi-GPU setups; for larger, distributed architectures, MultiWorkerMirroredStrategy or parameter servers are preferred based on the model and hardware constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
