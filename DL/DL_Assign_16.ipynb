{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b188c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Activation Functions\n",
    "# a) Sigmoid\n",
    "# The sigmoid function maps any input to a value between 0 and 1. It has an \"S\" shaped curve and is useful for binary classification tasks. However, it can lead to issues like vanishing gradients, where very high or low input values cause the gradients to become very small, slowing down the learning process.\n",
    "\n",
    "# b) Tanh\n",
    "# The hyperbolic tangent (tanh) function is similar to the sigmoid but outputs values between -1 and 1. It effectively centers the data, which can lead to better convergence during training compared to the sigmoid. Like sigmoid, it can also suffer from vanishing gradients for extreme input values.\n",
    "\n",
    "# c) ReLU (Rectified Linear Unit)\n",
    "# ReLU is a simple activation function defined as ùëì(ùë•)=max‚Å°(0,ùë•)\n",
    "# f(x)=max(0,x). It outputs zero for negative inputs and passes positive inputs unchanged. This function helps mitigate the vanishing gradient problem and leads to faster training, but it can result in dead neurons where some neurons become inactive and never recover.\n",
    "\n",
    "# d) ELU (Exponential Linear Unit)\n",
    "# ELU is designed to address some of the shortcomings of ReLU. It outputs a small negative value for negative inputs (instead of zero) and increases smoothly for positive inputs. This helps prevent dead neurons and can improve learning speed, particularly in deeper networks.\n",
    "\n",
    "# e) Leaky ReLU\n",
    "# Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient (a \"leak\") for negative inputs, typically defined as ùëì(ùë•)=ùë•f(x)=x for ùë•>0x>0 and ùëì(ùë•)=ùõºùë•f(x)=Œ±x for ùë•‚â§0x‚â§0 (where ùõº\n",
    "# Œ± is a small constant). This helps keep the neurons active during training, reducing the risk of dead neurons.\n",
    "\n",
    "# f) Swish\n",
    "# Swish is a newer activation function defined as ùëì(ùë•)=ùë•‚ãÖsigmoid(ùë•)\n",
    "# f(x)=x‚ãÖsigmoid(x). It can output both negative and positive values and tends to work well in practice, often outperforming ReLU and its variants in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcccf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate in Optimizers\n",
    "# When you increase the learning rate, the model learns faster but may overshoot the optimal point, leading to divergence or instability. On the other hand, decreasing the learning rate slows down the training process, allowing the model to make smaller updates. While this can lead to more precise convergence, it may also result in longer training times and the risk of getting stuck in local minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6677328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Hidden Neurons\n",
    "# Increasing the number of hidden neurons can give the network more capacity to learn complex patterns in the data. However, it can also lead to overfitting, where the model learns noise instead of the underlying patterns. The model may perform well on the training data but poorly on unseen data due to its increased complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07ba967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Batch Size\n",
    "# Increasing the batch size can lead to more stable and accurate gradient estimates, which can improve the training speed and convergence. However, it also requires more memory and can reduce the generalization capability of the model, potentially leading to overfitting. Smaller batch sizes introduce more noise in the gradients, which can help the model generalize better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10b87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization to Avoid Overfitting\n",
    "# Regularization techniques (like L1, L2 regularization, and dropout) are adopted to prevent overfitting by adding a penalty for more complex models. This encourages the model to focus on the most significant features, reduces reliance on noise in the training data, and promotes generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48450e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loss and Cost Functions\n",
    "# In deep learning, a loss function measures how well the model's predictions match the true labels for a single training example, while a cost function is the average of the loss over the entire dataset or a batch of examples. The goal of training is to minimize the cost function, which typically reflects the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74eca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Underfitting\n",
    "# Underfitting occurs when a model is too simple to capture the underlying patterns in the data. This can happen if the model has too few parameters, inadequate training, or an inappropriate model architecture. An underfitted model performs poorly on both training and test data, failing to achieve good accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2b4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout in Neural Networks\n",
    "# Dropout is a regularization technique used to prevent overfitting by randomly dropping a fraction of neurons during training. This forces the network to learn robust features that are not reliant on any single neuron, promoting better generalization. During inference, all neurons are used, ensuring the model has learned a comprehensive representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e586946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
