{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d9cfafb5",
   "metadata": {},
   "source": [
    "1. Stateful vs. Stateless RNNs:\n",
    "\n",
    "Stateful RNN:\n",
    "Pros: Retains hidden states across batch boundaries, ideal for sequences split across multiple batches (like time series).\n",
    "Cons: Can be challenging to reset states at the right times, and maintaining state consistency between batches requires more careful data handling.\n",
    "Stateless RNN:\n",
    "Pros: Resets hidden states after each batch, making it simpler to manage and faster for independent sequence processing.\n",
    "Cons: Not as suitable for long dependencies spanning multiple batches.\n",
    "\n",
    "2. Encoder–Decoder RNNs in Automatic Translation:\n",
    "\n",
    "Encoder–Decoder RNNs allow for flexible input and output lengths by encoding the entire input sequence into a fixed-size context vector. The decoder then generates the target sequence from this context, which provides more adaptability for varying sentence lengths and captures long-term dependencies better than a plain Seq2Seq RNN.\n",
    "\n",
    "3. Handling Variable-Length Sequences:\n",
    "\n",
    "Variable-Length Inputs: Use padding to equalize sequence lengths and apply masking so that padded elements are ignored during training.\n",
    "Variable-Length Outputs: Implement end-of-sequence tokens (<EOS>), so the model knows when to stop generating output, making it adaptable to different output lengths.\n",
    "\n",
    "4. Beam Search:\n",
    "\n",
    "Beam search is a decoding algorithm used to generate sequences with multiple possibilities, retaining the top sequences at each step. This helps in tasks like machine translation by finding higher-quality sequences. Libraries like TensorFlow’s beam_search and the Hugging Face transformers library support this.\n",
    "\n",
    "5. Attention Mechanism:\n",
    "\n",
    "Attention allows the model to focus on relevant parts of the input sequence at each decoding step, instead of relying on a single fixed-size context vector. This improves performance in tasks like translation by dynamically weighing parts of the input sequence based on relevance, enabling better handling of long sentences and complex dependencies.\n",
    "\n",
    "6. Key Layer in Transformer Architecture:\n",
    "\n",
    "The self-attention layer is crucial in the Transformer, allowing each token to interact with all others in the sequence. This creates context-rich embeddings and allows the model to understand relationships within the sequence independently of distance.\n",
    "\n",
    "7. Sampled Softmax:\n",
    "\n",
    "Sampled softmax reduces computation by only sampling a subset of classes to compute the softmax, which is especially useful for tasks with a large vocabulary (e.g., language models) where computing full softmax can be prohibitively slow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
