{
 "cells": [
  {
   "cell_type": "raw",
   "id": "af8bdcc9",
   "metadata": {},
   "source": [
    "1 No, it’s not OK to initialize all weights to the same value, even if it’s random, because all neurons would learn the same features. Different initial values enable each neuron to learn distinct patterns, which is critical for deep networks.\n",
    "Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "2 Yes, initializing biases to zero is fine. Biases help offset the weighted sum of inputs, and unlike weights, they don’t cause neurons to learn the same features if set to zero.\n",
    "Advantages of SELU over ReLU\n",
    "\n",
    "3 Self-normalizing: SELU (Scaled Exponential Linear Unit) self-normalizes, helping stabilize the variance of activations across layers.\n",
    "Improved gradient flow: SELU can reduce vanishing or exploding gradients during training.\n",
    "Better performance in deep networks: SELU’s self-normalization typically leads to faster convergence and improved performance in very deep networks.\n",
    "When to use various activation functions?\n",
    "\n",
    "4 SELU: For very deep networks where self-normalization is desired, especially when the network uses dense layers.\n",
    "Leaky ReLU and variants: For networks where ReLU’s “dead neuron” problem (neuron stops learning) is a risk. Leaky ReLU helps prevent neurons from dying by allowing a small gradient flow for negative values.\n",
    "ReLU: Widely used for most hidden layers in deep networks due to simplicity and efficiency.\n",
    "Tanh: When zero-centered outputs are required, and gradient vanishing isn’t a major concern.\n",
    "Logistic: Often used in the output layer for binary classification.\n",
    "Softmax: Used in the output layer for multi-class classification to produce probability distributions over classes.\n",
    "\n",
    "5 Effect of setting momentum too close to 1 in SGD?\n",
    "\n",
    "Setting the momentum hyperparameter too high (e.g., 0.99999) could cause oscillations or even divergence because the updates would rely too heavily on past gradients, causing the model to overshoot the minimum.\n",
    "Three ways to produce a sparse model\n",
    "\n",
    "6 L1 regularization: Encourages weights to become zero, creating sparsity.\n",
    "Pruning: Trims unimportant weights or neurons after training, making the model sparse.\n",
    "Sparse constraints: Constrains certain layers (e.g., sparse convolutional layers) to have sparse connections.\n",
    "Does dropout slow down training or inference? What about MC Dropout?\n",
    "\n",
    "7 Training: Dropout slightly slows down training because it deactivates neurons randomly.\n",
    "Inference: No, standard dropout doesn’t affect inference speed since dropout is disabled during prediction.\n",
    "MC Dropout: Using dropout during inference (MC Dropout) for uncertainty estimation slows down inference because it requires multiple passes for each prediction.\n",
    "\n",
    "\n",
    "8 Training a Deep Neural Network on CIFAR10\n",
    "Step-by-Step Process:\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each:\n",
    "Initialize with He initialization and use the ELU activation function for improved gradient flow and convergence.\n",
    "b. Train with Nadam optimizer and early stopping:\n",
    "Set up early stopping to prevent overfitting and use Nadam optimization for adaptive learning rates. CIFAR10 can be loaded with keras.datasets.cifar10.load_data() and has 50,000 training and 10,000 testing images, with 10 classes (use a softmax layer with 10 neurons).\n",
    "c. Add Batch Normalization:\n",
    "Compare learning curves: Batch normalization often speeds up convergence and can yield a better-performing model by stabilizing activations and reducing the need for lower learning rates.\n",
    "d. Replace Batch Normalization with SELU:\n",
    "Adjust to ensure self-normalization by standardizing inputs, using LeCun normal initialization, and including only dense layers to facilitate effective self-normalization.\n",
    "e. Regularize with Alpha Dropout and test MC Dropout for better accuracy:\n",
    "Alpha Dropout with SELU maintains mean and variance, enabling the network to self-normalize. Try MC Dropout to estimate model uncertainty and potentially achieve better accuracy without retraining.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
