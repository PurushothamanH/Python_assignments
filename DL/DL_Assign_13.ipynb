{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7b5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Prefer Logistic Regression over a Perceptron?\n",
    "\n",
    "# Preference: Logistic Regression offers probabilistic outputs and is better suited for handling non-linearly separable data by using a sigmoid activation function and optimizing via gradient descent. The Perceptron algorithm, by contrast, produces binary output, often failing to converge for non-linear data.\n",
    "# Tweaking a Perceptron: To make a Perceptron equivalent to Logistic Regression, replace the threshold step function with a sigmoid activation function and use gradient descent for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29997ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance of Logistic Activation in Training Early MLPs:\n",
    "\n",
    "# The logistic activation function (sigmoid) introduced non-linearity, enabling MLPs to learn complex patterns beyond linear separability. It also allowed for differentiable functions, crucial for backpropagation, enabling efficient gradient-based learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b909c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three Popular Activation Functions:\n",
    "\n",
    "# Sigmoid: Squashes input to the (0,1) range.\n",
    "# ReLU (Rectified Linear Unit): Outputs zero for negative inputs and linear values for positive inputs.\n",
    "# Tanh: Squashes input to the (-1,1) range, generally preferred over sigmoid due to zero-centered outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742696cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of Input Matrix 𝑋 X: If there are 𝑛n samples, each with 10 features, 𝑋X has a shape of (𝑛,10)(n,10).Shape of Hidden Layer’s Weight Vector 𝑊ℎW h​ : Since 𝑊ℎW h​  maps from 10 input features to 50 hidden neurons, it has a shape of (10,50)(10,50).\n",
    "# Shape of Hidden Layer’s Bias Vector 𝑏ℎb h​ : One bias per hidden neuron, shape is (50,)(50,).Shape of Output Layer’s Weight Vector 𝑊𝑜W o​: Maps from 50 hidden neurons to 3 output neurons, shape is (50,3)(50,3).Shape of Output Layer’s Bias Vector 𝑏𝑜b o​ : One bias per output neuron, shape is (3,)(3,)Shape of Network’s Output Matrix 𝑌Y: Final output matrix shape is (𝑛,3)(n,3) (one row per sample and one column per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4876b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Layer Neurons and Activation Functions:\n",
    "\n",
    "# Spam or Ham Classification: Use 1 output neuron with a sigmoid activation function (output 0 for ham, 1 for spam).\n",
    "# MNIST Classification: Use 10 output neurons (one per digit) with softmax activation to output class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a130f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation and Reverse-Mode Autodiff:\n",
    "\n",
    "# Backpropagation: A method for calculating gradients in neural networks by propagating the error backward through the layers, adjusting weights to minimize loss.\n",
    "# Difference: Backpropagation is a specific application of reverse-mode autodiff, where gradients are computed with respect to intermediate variables. Reverse-mode autodiff can be used more generally for any computation graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94536237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to Tweak in an MLP:\n",
    "\n",
    "# Hyperparameters: Number of layers, number of neurons per layer, learning rate, activation functions, batch size, number of epochs, dropout rate, regularization strength, optimizer choice.\n",
    "# Handling Overfitting: Reduce the number of neurons/layers, add dropout, increase regularization, or use early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52961fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8504 - loss: 0.5115 - val_accuracy: 0.9653 - val_loss: 0.1262\n",
      "Epoch 2/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.1707 - val_accuracy: 0.9742 - val_loss: 0.0922\n",
      "Epoch 3/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1221 - val_accuracy: 0.9763 - val_loss: 0.0811\n",
      "Epoch 4/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.0954 - val_accuracy: 0.9768 - val_loss: 0.0785\n",
      "Epoch 5/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.0811 - val_accuracy: 0.9807 - val_loss: 0.0724\n",
      "Epoch 6/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0682 - val_accuracy: 0.9810 - val_loss: 0.0707\n",
      "Epoch 7/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0585 - val_accuracy: 0.9825 - val_loss: 0.0712\n",
      "Epoch 8/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0551 - val_accuracy: 0.9805 - val_loss: 0.0704\n",
      "Epoch 9/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0480 - val_accuracy: 0.9828 - val_loss: 0.0713\n",
      "Epoch 10/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0463 - val_accuracy: 0.9842 - val_loss: 0.0687\n",
      "Epoch 11/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0401 - val_accuracy: 0.9833 - val_loss: 0.0745\n",
      "Epoch 12/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0382 - val_accuracy: 0.9835 - val_loss: 0.0750\n",
      "Epoch 13/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0343 - val_accuracy: 0.9823 - val_loss: 0.0756\n",
      "Epoch 14/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0358 - val_accuracy: 0.9820 - val_loss: 0.0788\n",
      "Epoch 15/20\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9879 - loss: 0.0348 - val_accuracy: 0.9830 - val_loss: 0.0779\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9754 - loss: 0.0790\n",
      "Test accuracy: 97.97%\n"
     ]
    }
   ],
   "source": [
    "# Training a Deep MLP on MNIST with 98% Precision:\n",
    "\n",
    "# Steps:\n",
    "# Build the model architecture with multiple hidden layers.\n",
    "# Use early stopping and dropout for regularization.\n",
    "# Save checkpoints during training.\n",
    "# Log metrics in TensorBoard for tracking.\n",
    "# Visualize learning curves and accuracy to assess model performance.\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess the MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train with checkpoint saving\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"mnist_model.keras\", save_best_only=True)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2abd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598d019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb192b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
