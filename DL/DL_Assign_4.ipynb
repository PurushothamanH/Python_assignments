{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0dce3882",
   "metadata": {},
   "source": [
    "1. How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?\n",
    "\n",
    "Description: TensorFlow is an open-source platform for building, training, and deploying machine learning and deep learning models.\n",
    "Main Features:\n",
    "Automatic differentiation: TensorFlow’s autograd functionality simplifies gradient computation.\n",
    "GPU/TPU support: Efficient performance scaling on GPUs and TPUs.\n",
    "Versatile API: High-level Keras API for easy model building, with low-level customization when needed.\n",
    "End-to-End Support: Tools for model visualization (TensorBoard), production deployment (TensorFlow Serving, TFX), and mobile support (TensorFlow Lite).\n",
    "Other Libraries: PyTorch, Keras, MXNet, Caffe, Theano, and JAX.\n",
    "\n",
    "2. Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?\n",
    "\n",
    "No, TensorFlow is not a drop-in replacement for NumPy. While it offers similar tensor operations and even includes tf.experimental.numpy for some compatibility, TensorFlow is optimized for large-scale machine learning with features like GPU support, automatic differentiation, and distributed computing. NumPy, in contrast, is focused on numerical computing for smaller-scale computations and doesn’t support automatic differentiation or GPU operations.\n",
    "\n",
    "3. Do you get the same result with tf.range(10) and tf.constant(np.arange(10))?\n",
    "\n",
    "Yes, both will produce a TensorFlow tensor with values [0, 1, 2, ..., 9], but tf.range(10) directly creates a TensorFlow tensor, while tf.constant(np.arange(10)) first creates a NumPy array and then converts it to a TensorFlow tensor.\n",
    "\n",
    "4. Six other data structures in TensorFlow, beyond regular tensors:\n",
    "\n",
    "tf.SparseTensor: For sparse data storage.\n",
    "tf.RaggedTensor: Handles variable-length lists within tensors.\n",
    "tf.TensorArray: For dynamic-sized tensors in loops.\n",
    "tf.data.Dataset: Manages data input pipelines.\n",
    "tf.Variable: For mutable tensors, often used in training.\n",
    "tf.string and tf.bool: Specialized data types for handling text and boolean data.\n",
    "\n",
    "5. Custom loss function: function vs subclassing keras.losses.Loss\n",
    "\n",
    "Function: Suitable for simple loss definitions without additional state or complex calculations.\n",
    "Subclassing Loss: Useful for more complex loss functions that require extra state or additional methods, like custom initialization or regularization.\n",
    "\n",
    "6. Custom metric: function vs subclassing keras.metrics.Metric\n",
    "\n",
    "Function: Best for straightforward metrics that don’t require internal state.\n",
    "Subclassing Metric: Ideal when your metric needs to maintain state (e.g., moving averages, counts) across batches.\n",
    "\n",
    "7. When to create a custom layer vs. a custom model?\n",
    "\n",
    "Custom Layer: When you need a reusable computation unit that performs specific tasks like a specialized activation or convolution operation.\n",
    "Custom Model: When defining an entire network architecture, especially if it involves multiple custom layers, unique training logic, or complex forward-pass operations.\n",
    "8. Use cases for custom training loops:\n",
    "\n",
    "When using dynamic models, specialized losses, or custom gradient calculations.\n",
    "Reinforcement learning or generative adversarial networks (GANs) often require custom training loops for updating each part independently.\n",
    "Fine-grained control for multi-task learning setups or custom optimization logic.\n",
    "9. Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?\n",
    "\n",
    "Custom Keras components generally need to be convertible to TF Functions to ensure compatibility with TensorFlow’s graph execution, especially when training on GPUs or TPUs. However, for debugging or prototyping, they can include arbitrary Python code (just note it may limit performance).\n",
    "\n",
    "10. Rules for a function to be convertible to a TF Function:\n",
    "\n",
    "Avoid side effects: Don’t modify Python variables or global states outside of the function.\n",
    "Use TensorFlow operations only: Only use operations that can be traced, like tf.* methods.\n",
    "Avoid dynamic control flow: Avoid non-TensorFlow control flow (use tf.while_loop and tf.cond instead of Python loops or conditionals).\n",
    "\n",
    "11. Dynamic Keras models: when, how, and why not always?\n",
    "\n",
    "When: Needed when the model has unpredictable behavior, like dynamic architectures that change based on input (e.g., recursive models or variable-length RNNs).\n",
    "How: By setting dynamic=True in the tf.keras.Model subclass.\n",
    "Why not always: Dynamic models lack graph optimizations available in static graphs, making them slower on devices like GPUs and TPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
