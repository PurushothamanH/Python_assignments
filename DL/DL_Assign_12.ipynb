{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469c53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsqueeze adds a dimension of size 1 to a tensor, allowing it to broadcast along a new axis. This is useful when you need to align dimensions for operations without changing the tensor’s data.\n",
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "a_unsqueezed = a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35acfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Indexing to Achieve unsqueeze:\n",
    "# Instead of unsqueeze, you can add a dimension by indexing with None at the desired axis.\n",
    "a = torch.tensor([1, 2, 3])\n",
    "a_unsqueezed = a[None, :]  # Equivalent to a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792c0ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      "[torch.storage._TypedStorage(dtype=torch.int64, device=cpu) of size 4]\n"
     ]
    }
   ],
   "source": [
    "# Showing Memory Content of a Tensor:\n",
    "# Use the storage attribute in PyTorch to access the underlying memory of a tensor.\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4562de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Adding a Vector of Size 3 to a 3×3 Matrix:\n",
    "# When broadcasting a vector to a 3×3 matrix, the vector elements are added to each row of the matrix:\n",
    "\n",
    "matrix = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "result = matrix + vector\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5506973",
   "metadata": {},
   "source": [
    "Do broadcasting and expand_as Increase Memory Use?:\n",
    "\n",
    "No, broadcasting and expand_as do not increase memory use. They create views by virtually repeating elements without allocating new memory, making operations more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf930df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement matmul Using Einstein Summation:\n",
    "# Einstein summation (einsum) provides a flexible way to express tensor operations, such as matrix multiplication, using index notation.\n",
    "\n",
    "import torch\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "result = torch.einsum('ij,jk->ik', a, b)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "163b86e8",
   "metadata": {},
   "source": [
    "Meaning of a Repeated Index in einsum:\n",
    "\n",
    "A repeated index on the left side of einsum notation represents a summation over that index. For example, ij,jk->ik implies summing over the j index."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9290f954",
   "metadata": {},
   "source": [
    "Three Rules of Einstein Summation Notation:\n",
    "\n",
    "Summing over repeated indices: Repeated indices imply summation over that dimension.\n",
    "Unused indices determine output shape: Unpaired indices appear in the output tensor.\n",
    "Non-repeated indices are preserved: Only dimensions without summation remain in the result."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4b37451",
   "metadata": {},
   "source": [
    "Forward and Backward Passes in a Neural Network:\n",
    "\n",
    "Forward pass: Activations flow from the input layer to the output layer, and predictions are generated.\n",
    "Backward pass: Gradients are propagated from the output layer back to the input layer, allowing the model to learn by adjusting weights.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85e91879",
   "metadata": {},
   "source": [
    "Why Store Activations During the Forward Pass?:\n",
    "\n",
    "Activations from intermediate layers are stored during the forward pass because they are needed to calculate gradients during the backward pass. Without these, gradient computation would be impossible, hindering weight updates.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a87ef3a9",
   "metadata": {},
   "source": [
    "Downside of Activations with Standard Deviation Far from 1:\n",
    "\n",
    "If activations have a standard deviation significantly higher or lower than 1, gradients can explode or vanish. This can lead to slow convergence or non-convergence during training.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ff1e07d",
   "metadata": {},
   "source": [
    "How Weight Initialization Can Help:\n",
    "\n",
    "Proper weight initialization, such as Xavier or He initialization, helps keep the standard deviation of activations close to 1. This stabilizes gradient magnitudes, allowing more efficient training and avoiding issues like exploding or vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adf175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
